# rag.py
"""
RAG ç³»ç»Ÿæ ¸å¿ƒé€»è¾‘

èŒè´£ï¼š
1. åŠ è½½å‘é‡æ•°æ®åº“
2. æ„å»º Retriever
3. ç»„è£… Prompt + LLM
4. æä¾›å¯æµå¼è¾“å‡ºçš„ RAG Chain
"""

from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from config import (
    VECTOR_DB_DIR,
    EMBED_MODEL,
    LLM_MODEL,
    TOP_K,
    TEMPERATURE
)
from prompts import RAG_PROMPT

def build_rag_chain(streaming: bool = False):
    print("ğŸ”§ åˆå§‹åŒ– RAG ç³»ç»Ÿ...")

    # ---------- 1. Embedding ----------
    embeddings = OllamaEmbeddings(model=EMBED_MODEL)

    # ---------- 2. å‘é‡æ•°æ®åº“ ----------
    print("ğŸ“š åŠ è½½å‘é‡æ•°æ®åº“:", VECTOR_DB_DIR)
    vectorstore = Chroma(
        persist_directory=VECTOR_DB_DIR,
        embedding_function=embeddings
    )

    # ---------- 3. Retriever ----------
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": TOP_K}
    )

    # ---------- 4. LLM ----------
    llm = OllamaLLM(
        model=LLM_MODEL,
        temperature=TEMPERATURE,
        streaming=streaming
    )

    # ---------- 5. RAG Chain ----------
    rag_chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | RAG_PROMPT
        | llm
        | StrOutputParser()
    )

    print("âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    return rag_chain