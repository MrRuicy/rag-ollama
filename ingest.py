# ingest.py
"""
æ–‡æ¡£å…¥åº“æ¨¡å— - æ”¯æŒå¤šæ ¼å¼æ–‡æ¡£
ä¿®å¤ç‰ˆæœ¬ï¼šä¿®å¤ç¼–ç é—®é¢˜å’Œmetadataè¿‡æ»¤
"""

import os
import re
import sys
import warnings
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# å¿½ç•¥æŸäº›è­¦å‘Š
warnings.filterwarnings("ignore")

# æ·»åŠ è¿‡æ»¤metadataçš„å·¥å…·
from langchain_community.vectorstores.utils import filter_complex_metadata

try:
    # LangChainæ–‡æ¡£åŠ è½½å™¨
    from langchain_community.document_loaders import (
        TextLoader,
        UnstructuredMarkdownLoader,
        PythonLoader,
        CSVLoader,
        UnstructuredHTMLLoader,
        JSONLoader,
        UnstructuredWordDocumentLoader,
        UnstructuredPowerPointLoader,
        UnstructuredExcelLoader,
        UnstructuredPDFLoader,
        UnstructuredFileLoader,
        DirectoryLoader
    )
    # æ–‡æœ¬åˆ†å‰²å™¨
    from langchain_text_splitters import (
        RecursiveCharacterTextSplitter,
        MarkdownHeaderTextSplitter,
        Language
    )
    # å‘é‡åŒ–æ¨¡å‹
    from langchain_ollama import OllamaEmbeddings
    from langchain_chroma import Chroma
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–ï¼špip install langchain-community langchain-text-splitters langchain-ollama langchain-chroma")
    sys.exit(1)

# OCRä¾èµ–ï¼ˆå¯é€‰ï¼‰
try:
    import pytesseract
    from PIL import Image
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False

# è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
try:
    from config import (
        DATA_DIR,
        EMBED_MODEL,
        VECTOR_DB_DIR,
        CHUNK_SIZE,
        CHUNK_OVERLAP
    )
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° config.pyï¼Œè¯·ç¡®ä¿ config.py æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)

def sanitize_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ¸…ç†metadataï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ChromaDBæ”¯æŒçš„ç±»å‹
    ChromaDBæ”¯æŒçš„ç±»å‹ï¼šstr, int, float, bool, None
    """
    sanitized = {}
    for key, value in metadata.items():
        if value is None:
            sanitized[key] = None
        elif isinstance(value, (str, int, float, bool)):
            sanitized[key] = value
        elif isinstance(value, list):
            # åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
            if all(isinstance(item, (str, int, float, bool)) for item in value):
                sanitized[key] = ", ".join(str(item) for item in value)
            else:
                sanitized[key] = str(value)
        else:
            # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            sanitized[key] = str(value)
    return sanitized

class MultiFormatDocumentProcessor:
    """å¤šæ ¼å¼æ–‡æ¡£å¤„ç†å™¨"""
    
    # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼æ˜ å°„åˆ°å¯¹åº”çš„åŠ è½½å™¨
    SUPPORTED_FORMATS = {
        # æ–‡æœ¬æ–‡ä»¶ - æ”¯æŒå¤šç§ç¼–ç 
        '.txt': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        '.md': lambda file_path: UnstructuredMarkdownLoader(file_path, mode="elements"),
        '.markdown': lambda file_path: UnstructuredMarkdownLoader(file_path, mode="elements"),
        
        # ä»£ç æ–‡ä»¶
        '.py': lambda file_path: PythonLoader(file_path),
        '.js': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        '.java': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        '.cpp': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        '.c': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        
        # æ•°æ®æ–‡ä»¶
        '.csv': lambda file_path: CSVLoader(file_path, encoding='utf-8'),
        '.json': lambda file_path: JSONLoader(
            file_path=file_path,
            jq_schema='.',
            text_content=False,
            json_lines=False
        ),
        
        # æ ‡è®°è¯­è¨€
        '.html': lambda file_path: UnstructuredHTMLLoader(file_path),
        '.htm': lambda file_path: UnstructuredHTMLLoader(file_path),
        
        # é…ç½®æ–‡ä»¶
        '.yaml': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        '.yml': lambda file_path: TextLoader(file_path, encoding='utf-8', autodetect_encoding=True),
        
        # Officeæ–‡æ¡£
        '.docx': lambda file_path: UnstructuredWordDocumentLoader(file_path, mode="elements"),
        '.doc': lambda file_path: UnstructuredWordDocumentLoader(file_path, mode="elements"),
        '.pptx': lambda file_path: UnstructuredPowerPointLoader(file_path, mode="elements"),
        '.ppt': lambda file_path: UnstructuredPowerPointLoader(file_path, mode="elements"),
        '.xlsx': lambda file_path: UnstructuredExcelLoader(file_path, mode="elements"),
        '.xls': lambda file_path: UnstructuredExcelLoader(file_path, mode="elements"),
        
        # PDFæ–‡æ¡£ - ä½¿ç”¨fastç­–ç•¥ï¼Œå¯¹æ–‡æœ¬å‹PDFæ•ˆæœå¥½
        '.pdf': lambda file_path: UnstructuredPDFLoader(
            file_path, 
            mode="elements",
            strategy="fast"
        ),
    }
    
    # å›¾ç‰‡æ ¼å¼ï¼ˆéœ€è¦OCRï¼‰
    IMAGE_FORMATS = {
        '.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.tif', '.gif'
    }
    
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨"""
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "total_chunks": 0,
            "start_time": None,
            "end_time": None
        }
        
        # æ£€æŸ¥OCRå¯ç”¨æ€§
        self.ocr_available = OCR_AVAILABLE
        
    def _get_file_extension(self, file_path: str) -> str:
        """è·å–æ–‡ä»¶æ‰©å±•åï¼ˆå°å†™ï¼‰"""
        return Path(file_path).suffix.lower()
    
    def _extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """æå–æ–‡ä»¶å…ƒæ•°æ®"""
        path = Path(file_path)
        try:
            stat = path.stat()
            
            # åŸºæœ¬å…ƒæ•°æ®
            metadata = {
                "source": str(path.relative_to(DATA_DIR)) if DATA_DIR in str(path) else str(path),
                "filename": path.name,
                "extension": self._get_file_extension(file_path),
                "file_size": stat.st_size,
                "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "directory": str(path.parent.relative_to(DATA_DIR)) if DATA_DIR in str(path) else str(path.parent),
            }
            
            # æ ¹æ®æ‰©å±•åæ·»åŠ å†…å®¹ç±»å‹
            ext = metadata["extension"]
            if ext in ['.py', '.js', '.java', '.cpp', '.c']:
                metadata["content_type"] = "code"
            elif ext == '.md':
                metadata["content_type"] = "markdown"
            elif ext in ['.csv', '.json', '.xlsx', '.xls']:
                metadata["content_type"] = "data"
            elif ext in ['.docx', '.doc', '.pptx', '.ppt']:
                metadata["content_type"] = "office"
            elif ext == '.pdf':
                metadata["content_type"] = "pdf"
            elif ext in self.IMAGE_FORMATS:
                metadata["content_type"] = "image"
            else:
                metadata["content_type"] = "text"
                
            return sanitize_metadata(metadata)
            
        except Exception as e:
            print(f"   âš ï¸  æå–å…ƒæ•°æ®å¤±è´¥ {file_path}: {e}")
            # è¿”å›æœ€å°å…ƒæ•°æ®
            return sanitize_metadata({
                "source": str(path),
                "filename": path.name,
                "extension": self._get_file_extension(file_path),
            })
    
    def _load_text_file(self, file_path: str) -> List:
        """åŠ è½½æ–‡æœ¬æ–‡ä»¶"""
        try:
            extension = self._get_file_extension(file_path)
            
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥æ ¼å¼
            if extension not in self.SUPPORTED_FORMATS:
                print(f"   âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {extension}")
                return []
            
            print(f"      ğŸ”§ ä½¿ç”¨åŠ è½½å™¨å¤„ç†: {extension}")
            # è·å–åŠ è½½å™¨
            loader_func = self.SUPPORTED_FORMATS[extension]
            loader = loader_func(file_path)
            
            # åŠ è½½æ–‡æ¡£
            documents = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            base_metadata = self._extract_metadata(file_path)
            for doc in documents:
                if hasattr(doc, 'metadata'):
                    # æ¸…ç†åŸå§‹metadata
                    if doc.metadata:
                        doc.metadata = sanitize_metadata(doc.metadata)
                    # æ›´æ–°åŸºç¡€å…ƒæ•°æ®
                    doc.metadata.update(base_metadata)
                else:
                    doc.metadata = base_metadata
            
            print(f"      âœ… åŠ è½½æˆåŠŸï¼Œå¾—åˆ° {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            return documents
            
        except ImportError as e:
            print(f"   âŒ å¯¼å…¥é”™è¯¯: {e}")
            # æç¤ºå®‰è£…ä¾èµ–
            if extension == '.pdf':
                print("      è¯·å®‰è£…PDFæ”¯æŒ: pip install unstructured[pdf] pdfminer.six")
            elif extension == '.docx':
                print("      è¯·å®‰è£…Wordæ”¯æŒ: pip install unstructured[docx]")
            return []
            
        except Exception as e:
            print(f"   âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            
            # ç‰¹æ®Šå¤„ç†ï¼šå°è¯•ç”¨ä¸åŒç¼–ç æ‰“å¼€æ–‡æœ¬æ–‡ä»¶
            if extension == '.txt':
                print("      ğŸ”„ å°è¯•å…¶ä»–ç¼–ç ...")
                try:
                    # å°è¯•å¸¸è§ç¼–ç 
                    encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin1', 'cp1252']
                    for encoding in encodings:
                        try:
                            with open(file_path, 'r', encoding=encoding) as f:
                                content = f.read()
                            # ä½¿ç”¨TextLoaderé‡æ–°åŠ è½½
                            from langchain_community.document_loaders import TextLoader
                            loader = TextLoader(file_path, encoding=encoding)
                            documents = loader.load()
                            
                            # æ·»åŠ å…ƒæ•°æ®
                            base_metadata = self._extract_metadata(file_path)
                            for doc in documents:
                                doc.metadata = base_metadata
                            
                            print(f"      âœ… ä½¿ç”¨ {encoding} ç¼–ç åŠ è½½æˆåŠŸ")
                            return documents
                        except UnicodeDecodeError:
                            continue
                    print("      âŒ å°è¯•æ‰€æœ‰ç¼–ç å‡å¤±è´¥")
                except Exception as e2:
                    print(f"      âŒ å¤‡ç”¨åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
            
            return []
    
    def _load_image_with_ocr(self, file_path: str) -> List:
        """ä½¿ç”¨OCRåŠ è½½å›¾ç‰‡æ–‡ä»¶"""
        if not self.ocr_available:
            print(f"   âš ï¸  OCRåŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡å›¾ç‰‡æ–‡ä»¶")
            return []
        
        try:
            # ä½¿ç”¨pytesseractç›´æ¥OCR
            import pytesseract
            from PIL import Image
            
            image = Image.open(file_path)
            text = pytesseract.image_to_string(image, lang='chi_sim+eng')
            
            if not text.strip():
                print(f"   âš ï¸  å›¾ç‰‡ä¸­æœªè¯†åˆ«åˆ°æ–‡å­—")
                return []
            
            from langchain_core.documents import Document
            documents = [Document(
                page_content=text,
                metadata=self._extract_metadata(file_path)
            )]
            
            print(f"   âœ…  OCRè¯†åˆ«æˆåŠŸï¼Œæå– {len(text)} å­—ç¬¦")
            return documents
            
        except Exception as e:
            print(f"   âŒ å›¾ç‰‡OCRå¤±è´¥: {e}")
            return []
    
    def _smart_text_splitter(self, documents: List) -> List:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨"""
        all_chunks = []
        
        for doc in documents:
            content = doc.page_content
            metadata = doc.metadata.copy()  # å¤åˆ¶metadata
            extension = metadata.get("extension", "")
            
            try:
                if extension == '.md':
                    # Markdownæ–‡ä»¶æŒ‰æ ‡é¢˜åˆ†å‰²
                    headers_to_split_on = [
                        ("#", "Header 1"),
                        ("##", "Header 2"),
                        ("###", "Header 3"),
                    ]
                    markdown_splitter = MarkdownHeaderTextSplitter(
                        headers_to_split_on=headers_to_split_on,
                        strip_headers=False
                    )
                    chunks = markdown_splitter.split_text(content)
                elif extension == '.py':
                    # Pythonä»£ç æŒ‰å‡½æ•°/ç±»åˆ†å‰²
                    python_splitter = RecursiveCharacterTextSplitter.from_language(
                        language=Language.PYTHON,
                        chunk_size=CHUNK_SIZE,
                        chunk_overlap=CHUNK_OVERLAP
                    )
                    chunks = python_splitter.split_documents([doc])
                else:
                    # é€šç”¨æ–‡æœ¬åˆ†å‰²å™¨
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=CHUNK_SIZE,
                        chunk_overlap=CHUNK_OVERLAP,
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""],
                        length_function=len,
                    )
                    chunks = splitter.split_documents([doc])
            except Exception as e:
                print(f"      âš ï¸  åˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†å‰²å™¨: {e}")
                # å¤‡ç”¨åˆ†å‰²å™¨
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=CHUNK_SIZE,
                    chunk_overlap=CHUNK_OVERLAP,
                    separators=["\n\n", "\n", " ", ""],
                )
                chunks = splitter.split_documents([doc])
            
            # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()
                chunk_metadata["chunk_id"] = f"{metadata.get('filename', 'doc')}_{i}"
                chunk_metadata["chunk_index"] = i
                chunk_metadata["total_chunks"] = len(chunks)
                chunk.metadata = sanitize_metadata(chunk_metadata)
            
            all_chunks.extend(chunks)
        
        return all_chunks
    
    def process_file(self, file_path: str) -> List:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        extension = self._get_file_extension(file_path)
        filename = Path(file_path).name
        
        print(f"   ğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        if extension in self.IMAGE_FORMATS:
            documents = self._load_image_with_ocr(file_path)
        else:
            documents = self._load_text_file(file_path)
        
        if not documents:
            self.stats["failed_files"] += 1
            return []
        
        # æ™ºèƒ½åˆ†å‰²
        chunks = self._smart_text_splitter(documents)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats["processed_files"] += 1
        self.stats["total_chunks"] += len(chunks)
        
        print(f"      âœ… æˆåŠŸåˆ†å‰²ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    required_packages = [
        "langchain",
        "langchain_community",
        "langchain_text_splitters",
        "chromadb",
        "unstructured",
    ]
    
    missing_required = []
    for package in required_packages:
        try:
            __import__(package.replace("-", "_"))
        except ImportError:
            missing_required.append(package)
    
    if missing_required:
        print("âŒ ç¼ºå°‘å¿…éœ€ä¾èµ–:")
        for package in missing_required:
            print(f"   - {package}")
        print(f"\nè¯·è¿è¡Œ: pip install {' '.join(missing_required)}")
        return False
    
    print("âœ… åŸºç¡€ä¾èµ–æ£€æŸ¥é€šè¿‡")
    return True

def ingest():
    """ä¸»å…¥åº“å‡½æ•°"""
    print("=" * 70)
    print("ğŸ“¥ å¤šæ ¼å¼æ–‡æ¡£å‘é‡åŒ–å…¥åº“ç³»ç»Ÿ")
    print("=" * 70)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = MultiFormatDocumentProcessor()
    processor.stats["start_time"] = datetime.now()
    
    # ---------- 1. æ‰«ææ–‡æ¡£ç›®å½• ----------
    print(f"\nğŸ“‚ æ‰«ææ–‡æ¡£ç›®å½•: {DATA_DIR}")
    
    if not os.path.exists(DATA_DIR):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
        print("   è¯·åˆ›å»ºdata/ç›®å½•å¹¶æ”¾å…¥æ–‡æ¡£")
        return
    
    # æ”¶é›†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    all_files = []
    supported_extensions = set(processor.SUPPORTED_FORMATS.keys()) | processor.IMAGE_FORMATS
    
    for ext in supported_extensions:
        pattern = f"**/*{ext}"
        files = list(Path(DATA_DIR).glob(pattern))
        all_files.extend(files)
    
    # å»é‡
    all_files = list(set(all_files))
    
    if not all_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡æ¡£")
        return
    
    processor.stats["total_files"] = len(all_files)
    print(f"âœ… æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    # ---------- 2. å¤„ç†æ‰€æœ‰æ–‡ä»¶ ----------
    print("\nğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£...")
    all_chunks = []
    
    for file_path in all_files:
        chunks = processor.process_file(str(file_path))
        if chunks:
            all_chunks.extend(chunks)
    
    # ---------- 3. æ£€æŸ¥å¤„ç†ç»“æœ ----------
    print(f"\n{'='*60}")
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   æ€»æ–‡ä»¶æ•°: {processor.stats['total_files']}")
    print(f"   æˆåŠŸå¤„ç†: {processor.stats['processed_files']}")
    print(f"   å¤±è´¥æ–‡ä»¶: {processor.stats['failed_files']}")
    print(f"   ç”Ÿæˆæ–‡æœ¬å—: {processor.stats['total_chunks']}")
    
    if processor.stats['total_chunks'] == 0:
        print("âŒ æœªç”Ÿæˆä»»ä½•æ–‡æœ¬å—")
        return
    
    # ---------- 4. åˆå§‹åŒ– Embedding ----------
    print(f"\nğŸ§  åˆå§‹åŒ– Embedding æ¨¡å‹: {EMBED_MODEL}")
    try:
        embeddings = OllamaEmbeddings(model=EMBED_MODEL)
        # ç®€å•æµ‹è¯•
        test_vector = embeddings.embed_query("test")
        print(f"âœ… Embeddingæ¨¡å‹å¯ç”¨ï¼Œå‘é‡ç»´åº¦: {len(test_vector)}")
    except Exception as e:
        print(f"âŒ Embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   è¯·ç¡®ä¿OllamaæœåŠ¡è¿è¡Œä¸”æ¨¡å‹å·²ä¸‹è½½")
        return
    
    # ---------- 5. æ„å»ºå‘é‡æ•°æ®åº“ ----------
    print(f"\nğŸ“¦ æ„å»º Chroma å‘é‡åº“...")
    print(f"   å­˜å‚¨ä½ç½®: {VECTOR_DB_DIR}")
    
    try:
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        os.makedirs(VECTOR_DB_DIR, exist_ok=True)
        
        # è¿‡æ»¤metadataä¸­çš„å¤æ‚ç±»å‹
        print(f"   ğŸ§¹ è¿‡æ»¤metadataä¸­çš„å¤æ‚ç±»å‹...")
        from langchain_community.vectorstores.utils import filter_complex_metadata
        filtered_chunks = filter_complex_metadata(all_chunks)
        print(f"   è¿‡æ»¤åå‰©ä½™ {len(filtered_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡åº“
        chroma_db_path = os.path.join(VECTOR_DB_DIR, "chroma.sqlite3")
        if os.path.exists(chroma_db_path):
            print("   ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰å‘é‡åº“ï¼Œè¿›è¡Œå¢é‡æ›´æ–°...")
            # åŠ è½½ç°æœ‰å‘é‡åº“
            vectorstore = Chroma(
                persist_directory=VECTOR_DB_DIR,
                embedding_function=embeddings
            )
            # æ·»åŠ æ–°æ–‡æ¡£
            vectorstore.add_documents(documents=filtered_chunks)
            print(f"   âœ… å¢é‡æ›´æ–°å®Œæˆ")
        else:
            # åˆ›å»ºæ–°å‘é‡åº“
            vectorstore = Chroma.from_documents(
                documents=filtered_chunks,
                embedding=embeddings,
                persist_directory=VECTOR_DB_DIR
            )
            print(f"   âœ… æ–°å»ºå‘é‡åº“å®Œæˆ")
        
        # ç»Ÿè®¡ä¿¡æ¯
        collection_count = vectorstore._collection.count()
        print(f"   å­˜å‚¨å‘é‡æ•°: {collection_count}")
        
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ---------- 6. å®Œæˆç»Ÿè®¡ ----------
    processor.stats["end_time"] = datetime.now()
    duration = (processor.stats["end_time"] - processor.stats["start_time"]).total_seconds()
    
    print(f"\nğŸ‰ å…¥åº“å®Œæˆ!")
    print(f"   æ€»è€—æ—¶: {duration:.1f} ç§’")
    print(f"   å¹³å‡é€Ÿåº¦: {processor.stats['total_chunks']/max(duration, 0.1):.1f} å—/ç§’")
    print(f"   å­˜å‚¨ä½ç½®: {VECTOR_DB_DIR}")

if __name__ == "__main__":
    # ç®€å•å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] in ["--help", "-h"]:
        print("ç”¨æ³•: python ingest.py")
        print("       python ingest.py --check  # æ£€æŸ¥ä¾èµ–")
    elif len(sys.argv) > 1 and sys.argv[1] == "--check":
        check_dependencies()
    else:
        ingest()