"""
æ°‘æ³•å…¸ä¸“ç”¨æ–‡æ¡£å¤„ç†å™¨
"""

import os
import re
import sys
import time
import warnings
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests

warnings.filterwarnings("ignore")

# æ·»åŠ æ¨¡å—è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ ¹ç›®å½•çš„é€šç”¨æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

try:
    from langchain_community.document_loaders import UnstructuredPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_ollama import OllamaEmbeddings
    from langchain_chroma import Chroma
    from langchain_community.vectorstores.utils import filter_complex_metadata
    from langchain_core.documents import Document
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install -r requirements_civil.txt")
    sys.exit(1)

# å¯¼å…¥ä¸“ç”¨é…ç½®
from examples.Civil_Code_RAG_Assistant.configs.civil_config import (
    CIVIL_DATA_DIR,
    CIVIL_VECTOR_DB_DIR,
    EMBED_MODEL,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    SPLIT_STRATEGY,
    CIVIL_CODE_SECTIONS
)

# å‚è€ƒé€šç”¨æ–‡ä»¶æ·»åŠ çš„è¾…åŠ©å‡½æ•°
def sanitize_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ¸…ç†metadataï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ChromaDBæ”¯æŒçš„ç±»å‹
    ChromaDBæ”¯æŒçš„ç±»å‹ï¼šstr, int, float, bool, None
    å‚è€ƒé€šç”¨æ–‡ä»¶çš„å®ç°
    """
    sanitized = {}
    for key, value in metadata.items():
        if value is None:
            sanitized[key] = None
        elif isinstance(value, (str, int, float, bool)):
            sanitized[key] = value
        elif isinstance(value, list):
            # åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
            if all(isinstance(item, (str, int, float, bool)) for item in value):
                sanitized[key] = ", ".join(str(item) for item in value)
            else:
                sanitized[key] = str(value)
        else:
            # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            sanitized[key] = str(value)
    return sanitized

class CivilCodeIngestor:
    """æ°‘æ³•å…¸ä¸“ç”¨æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.stats = {
            "total_pages": 0,
            "total_articles": 0,
            "total_chunks": 0,
            "sections_found": [],
            "start_time": 0,
            "end_time": 0,
            "ollama_retries": 0
        }
        
        # æ°‘æ³•å…¸ç»“æ„æ­£åˆ™æ¨¡å¼
        self.section_patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç¼–\s+([^\s]+)',  # ç¬¬Xç¼– ç« èŠ‚å
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« \s+([^\s]+)',  # ç¬¬Xç«  ç« èŠ‚å
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚\s+([^\s]+)',  # ç¬¬XèŠ‚ ç« èŠ‚å
        ]
        
        self.article_pattern = r'^ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡\s*(.*)'
        
        # Ollama æœåŠ¡é…ç½®
        self.ollama_host = "http://127.0.0.1:11434"
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    
    def check_ollama_service(self) -> bool:
        """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
        try:
            print("ğŸ” æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            if response.status_code == 200:
                print("âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
                models = response.json().get("models", [])
                model_names = [model.get("name") for model in models]
                
                # æ£€æŸ¥å®Œæ•´æ¨¡å‹åæˆ–åŸºç¡€å
                model_found = False
                for model_name in model_names:
                    if EMBED_MODEL == model_name or EMBED_MODEL.startswith(model_name.split(':')[0]):
                        model_found = True
                        print(f"âœ… æ¨¡å‹ '{EMBED_MODEL}' å·²æ‰¾åˆ° (å®é™…åç§°: {model_name})")
                        break
                
                if not model_found and model_names:
                    print(f"âŒ æ¨¡å‹ '{EMBED_MODEL}' æœªæ‰¾åˆ°")
                    print(f"   å¯ç”¨æ¨¡å‹: {', '.join(model_names)}")
                    print(f"\nğŸ’¡ è¯·ä¸‹è½½æ¨¡å‹: ollama pull {EMBED_MODEL.split(':')[0]}")
                    return False
                elif not model_names:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹")
                    print(f"ğŸ’¡ è¯·ä¸‹è½½æ¨¡å‹: ollama pull {EMBED_MODEL.split(':')[0]}")
                    return False
                    
                return True
            else:
                print(f"âŒ Ollama API è¿”å›é”™è¯¯: {response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print("âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
            print("ğŸ’¡ è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨:")
            print("   1. åœ¨ç»ˆç«¯è¿è¡Œ: ollama serve")
            print("   2. æˆ–è€… Windows: å¯åŠ¨ Ollama åº”ç”¨")
            return False
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ Ollama æœåŠ¡æ—¶å‡ºé”™: {e}")
            return False
    
    def start_ollama_service(self) -> bool:
        """å°è¯•å¯åŠ¨ Ollama æœåŠ¡"""
        print("ğŸ”„ å°è¯•å¯åŠ¨ Ollama æœåŠ¡...")
        
        try:
            # æ ¹æ®ä¸åŒæ“ä½œç³»ç»Ÿå°è¯•å¯åŠ¨
            if sys.platform == "win32":
                # Windows: å°è¯•å¯åŠ¨ Ollama åº”ç”¨
                result = subprocess.run(
                    ["ollama", "serve"],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                if result.returncode == 0:
                    print("âœ… Ollama æœåŠ¡å¯åŠ¨æˆåŠŸ")
                    time.sleep(5)  # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
                    return True
                else:
                    print(f"âŒ å¯åŠ¨å¤±è´¥: {result.stderr}")
                    return False
            else:
                # Linux/Mac: ä½¿ç”¨ systemd æˆ–ç›´æ¥å¯åŠ¨
                result = subprocess.run(
                    ["systemctl", "--user", "start", "ollama"],
                    capture_output=True,
                    text=True
                )
                if result.returncode == 0:
                    print("âœ… é€šè¿‡ systemctl å¯åŠ¨ Ollama")
                    time.sleep(5)
                    return True
                else:
                    print("âš ï¸  systemctl å¯åŠ¨å¤±è´¥ï¼Œå°è¯•ç›´æ¥å¯åŠ¨...")
                    # åå°å¯åŠ¨ ollama serve
                    subprocess.Popen(
                        ["ollama", "serve"],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL
                    )
                    print("âœ… Ollama æœåŠ¡å·²åœ¨åå°å¯åŠ¨")
                    time.sleep(10)  # ç»™æ›´å¤šæ—¶é—´å¯åŠ¨
                    return True
                    
        except FileNotFoundError:
            print("âŒ Ollama æœªå®‰è£…")
            print("ğŸ’¡ è¯·å…ˆå®‰è£… Ollama: https://ollama.com/download")
            return False
        except Exception as e:
            print(f"âŒ å¯åŠ¨ Ollama æœåŠ¡å¤±è´¥: {e}")
            return False
    
    def initialize_embeddings_with_retry(self) -> Optional[OllamaEmbeddings]:
        """åˆå§‹åŒ– Embedding æ¨¡å‹ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        max_retries = self.max_retries
        base_delay = 2  # åŸºç¡€å»¶è¿Ÿ2ç§’
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ§  åˆå§‹åŒ– Embedding æ¨¡å‹ ({attempt + 1}/{max_retries}): {EMBED_MODEL}")
                
                # æ ¹æ® langchain-ollama ç‰ˆæœ¬è°ƒæ•´å‚æ•°
                # æ–°ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒ timeout å‚æ•°ï¼Œä½¿ç”¨æ›´ç®€å•çš„åˆå§‹åŒ–
                try:
                    # å°è¯•ä¸å¸¦ timeout å‚æ•°
                    embeddings = OllamaEmbeddings(
                        model=EMBED_MODEL,
                        base_url=self.ollama_host
                    )
                except TypeError as e:
                    if "unexpected keyword argument 'timeout'" in str(e):
                        # å¦‚æœ timeout å‚æ•°ä¸è¢«æ”¯æŒï¼Œä½¿ç”¨æ›´ç®€å•çš„åˆå§‹åŒ–
                        print("   æ£€æµ‹åˆ°ä¸æ”¯æŒ timeout å‚æ•°ï¼Œä½¿ç”¨ç®€åŒ–åˆå§‹åŒ–...")
                        embeddings = OllamaEmbeddings(model=EMBED_MODEL)
                    else:
                        raise e
                
                # ç®€å•æµ‹è¯•è¿æ¥
                print("   æµ‹è¯•æ¨¡å‹è¿æ¥...")
                test_vector = embeddings.embed_query("æµ‹è¯•è¿æ¥")
                
                if not test_vector or len(test_vector) == 0:
                    raise ValueError("Embedding è¿”å›ç©ºå‘é‡")
                
                print(f"âœ… Embedding æ¨¡å‹å¯ç”¨ï¼Œå‘é‡ç»´åº¦: {len(test_vector)}")
                return embeddings
                
            except requests.exceptions.ConnectionError as e:
                self.stats["ollama_retries"] += 1
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    print(f"âš ï¸  è¿æ¥å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•... (é”™è¯¯: {str(e)[:100]})")
                    time.sleep(delay)
                    
                    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œå°è¯•é‡å¯æœåŠ¡
                    if attempt == 0:
                        if not self.check_ollama_service():
                            self.start_ollama_service()
                else:
                    print(f"âŒ è¿æ¥é‡è¯• {max_retries} æ¬¡å‡å¤±è´¥")
                    return None
                    
            except Exception as e:
                self.stats["ollama_retries"] += 1
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)
                    error_msg = str(e)
                    print(f"âš ï¸  åµŒå…¥å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•... (é”™è¯¯: {error_msg[:100]})")
                    time.sleep(delay)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹å®šé”™è¯¯
                    if "extra_forbidden" in error_msg and "timeout" in error_msg:
                        print("   æ£€æµ‹åˆ° timeout å‚æ•°é—®é¢˜ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–...")
                else:
                    print(f"âŒ åµŒå…¥é‡è¯• {max_retries} æ¬¡å‡å¤±è´¥: {e}")
                    return None
        
        return None
    
    def find_civil_code_pdf(self) -> Optional[Path]:
        """æŸ¥æ‰¾æ°‘æ³•å…¸PDFæ–‡ä»¶"""
        data_dir = Path(CIVIL_DATA_DIR)
        
        if not data_dir.exists():
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return None
        
        # æœç´¢PDFæ–‡ä»¶
        pdf_files = list(data_dir.glob("*.pdf"))
        
        if not pdf_files:
            print(f"âŒ åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return None
        
        # ä¼˜å…ˆæŸ¥æ‰¾åç§°åŒ…å«"æ°‘æ³•å…¸"çš„æ–‡ä»¶
        for pdf in pdf_files:
            if "æ°‘æ³•å…¸" in pdf.name or "civil" in pdf.name.lower():
                return pdf
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®å‘½åçš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªPDF
        print(f"âš ï¸  æœªæ‰¾åˆ°æ˜ç¡®å‘½åä¸º'æ°‘æ³•å…¸'çš„PDFï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ–‡ä»¶")
        return pdf_files[0]
    
    def extract_legal_structure(self, content: str) -> List[Dict[str, Any]]:
        """æå–æ°‘æ³•å…¸ç»“æ„"""
        lines = content.split('\n')
        structure = []
        current_section = "æ€»åˆ™"
        current_chapter = ""
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # æ£€æµ‹ç« èŠ‚
            for pattern in self.section_patterns:
                match = re.match(pattern, line)
                if match:
                    section_name = match.group(1)
                    if "ç¼–" in line:
                        current_section = section_name
                        if section_name not in self.stats["sections_found"]:
                            self.stats["sections_found"].append(section_name)
                    elif "ç« " in line:
                        current_chapter = section_name
                    
                    structure.append({
                        "type": "section",
                        "name": section_name,
                        "line": i,
                        "full_path": f"{current_section} - {current_chapter} - {section_name}" 
                                      if current_chapter else f"{current_section} - {section_name}"
                    })
                    break
            
            # æ£€æµ‹æ³•æ¡
            match = re.match(self.article_pattern, line)
            if match:
                article_content = match.group(1)
                structure.append({
                    "type": "article",
                    "number": line.split('æ¡')[0] + 'æ¡',
                    "content_preview": article_content[:50] + "..." if len(article_content) > 50 else article_content,
                    "line": i,
                    "section": current_section,
                    "chapter": current_chapter,
                    "is_article": True
                })
                self.stats["total_articles"] += 1
        
        return structure
    
    def split_by_legal_structure(self, content: str) -> List[Document]:
        """æŒ‰æ³•å¾‹ç»“æ„åˆ†å‰²æ–‡æ¡£"""
        print("   ä½¿ç”¨æ³•å¾‹ç»“æ„æ„ŸçŸ¥åˆ†å‰²ç­–ç•¥...")
        
        structure = self.extract_legal_structure(content)
        
        if not structure:
            print("   âš ï¸  æœªèƒ½æ£€æµ‹åˆ°æ³•å¾‹ç»“æ„ï¼Œä½¿ç”¨é€šç”¨åˆ†å‰²")
            return self.split_generic(content)
        
        lines = content.split('\n')
        chunks = []
        current_chunk_lines = []
        
        # åŸºç¡€å…ƒæ•°æ®ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„å®ç°ï¼‰
        current_metadata = sanitize_metadata({
            "document_type": "æ°‘æ³•å…¸",
            "law_type": "civil",
            "country": "ä¸­å›½",
            "year": "2021",
            "content_type": "law_document"
        })
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æ„æ ‡è®°
            is_structure = False
            for struct_item in structure:
                if struct_item["line"] == i:
                    is_structure = True
                    
                    # ä¿å­˜å½“å‰chunkï¼ˆå¦‚æœæœ‰è¶³å¤Ÿå†…å®¹ï¼‰
                    if current_chunk_lines and len(''.join(current_chunk_lines)) > 100:
                        chunk_text = '\n'.join(current_chunk_lines)
                        doc = Document(
                            page_content=chunk_text,
                            metadata=current_metadata.copy()
                        )
                        chunks.append(doc)
                        current_chunk_lines = []
                    
                    # æ›´æ–°metadataï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„sanitize_metadataï¼‰
                    if struct_item["type"] == "article":
                        current_metadata.update(sanitize_metadata({
                            "article_number": struct_item["number"],
                            "section": struct_item["section"],
                            "chapter": struct_item.get("chapter", ""),
                            "content_type": "law_article",
                            "is_law_article": True
                        }))
                    elif struct_item["type"] == "section":
                        current_metadata.update(sanitize_metadata({
                            "section_name": struct_item["name"],
                            "full_path": struct_item.get("full_path", ""),
                            "content_type": "section_header"
                        }))
                    
                    current_chunk_lines.append(line)
                    break
            
            if not is_structure and line:
                current_chunk_lines.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªchunk
        if current_chunk_lines and len(''.join(current_chunk_lines)) > 100:
            chunk_text = '\n'.join(current_chunk_lines)
            doc = Document(
                page_content=chunk_text,
                metadata=current_metadata.copy()
            )
            chunks.append(doc)
        
        return chunks
    
    def split_generic(self, content: str) -> List[Document]:
        """é€šç”¨åˆ†å‰²ï¼ˆå¤‡ç”¨ï¼‰"""
        print("   ä½¿ç”¨é€šç”¨åˆ†å‰²ç­–ç•¥...")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\nç¬¬", "\nç¬¬", "\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " "],
            length_function=len,
        )
        
        doc = Document(page_content=content)
        chunks = splitter.split_documents([doc])
        
        # æ·»åŠ æ³•å¾‹ç›¸å…³å…ƒæ•°æ®ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„sanitize_metadataï¼‰
        base_metadata = sanitize_metadata({
            "document_type": "æ°‘æ³•å…¸",
            "law_type": "civil",
            "content_type": "generic_split",
            "country": "ä¸­å›½",
            "year": "2021"
        })
        
        for chunk in chunks:
            chunk.metadata.update(base_metadata)
        
        return chunks
    
    def _extract_pdf_metadata(self, pdf_path: Path) -> Dict[str, Any]:
        """æå–PDFæ–‡ä»¶å…ƒæ•°æ®ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„å®ç°ï¼‰"""
        try:
            stat = pdf_path.stat()
            metadata = sanitize_metadata({
                "source": str(pdf_path.name),
                "filename": pdf_path.name,
                "extension": ".pdf",
                "file_size": stat.st_size,
                "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "content_type": "pdf",
                "document_type": "æ°‘æ³•å…¸",
                "law_type": "civil"
            })
            return metadata
        except Exception as e:
            print(f"   âš ï¸  æå–PDFå…ƒæ•°æ®å¤±è´¥: {e}")
            return sanitize_metadata({
                "source": str(pdf_path.name),
                "filename": pdf_path.name,
                "extension": ".pdf",
                "content_type": "pdf"
            })
    
    def _load_pdf_with_retry(self, pdf_path: Path) -> List[Document]:
        """åŠ è½½PDFæ–‡ä»¶ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„å¤šæ ¼å¼å¤„ç†ï¼‰"""
        pdf_metadata = self._extract_pdf_metadata(pdf_path)
        
        # å°è¯•ä¸åŒçš„åŠ è½½ç­–ç•¥
        strategies = [
            # ç­–ç•¥1: ä½¿ç”¨fastç­–ç•¥ï¼ˆå¯¹æ–‡æœ¬å‹PDFæ•ˆæœå¥½ï¼‰
            ("fast", {"mode": "elements", "strategy": "fast"}),
            # ç­–ç•¥2: ä½¿ç”¨hi_resç­–ç•¥ï¼ˆå¯¹æ‰«æPDFæ•ˆæœå¥½ï¼‰
            ("hi_res", {"mode": "elements", "strategy": "hi_res"}),
            # ç­–ç•¥3: ä½¿ç”¨pagedç­–ç•¥ï¼ˆåˆ†é¡µåŠ è½½ï¼‰
            ("paged", {"mode": "paged", "strategy": "auto"})
        ]
        
        for strategy_name, loader_params in strategies:
            try:
                print(f"   å°è¯•ç­–ç•¥: {strategy_name}")
                loader = UnstructuredPDFLoader(str(pdf_path), **loader_params)
                documents = loader.load()
                
                # æ·»åŠ å…ƒæ•°æ®åˆ°æ¯ä¸ªæ–‡æ¡£
                for doc in documents:
                    doc.metadata.update(pdf_metadata)
                    # ç¡®ä¿metadataè¢«æ¸…ç†
                    doc.metadata = sanitize_metadata(doc.metadata)
                
                print(f"      âœ… {strategy_name}ç­–ç•¥æˆåŠŸï¼Œå¾—åˆ° {len(documents)} ä¸ªå…ƒç´ ")
                return documents
                
            except Exception as e:
                print(f"      âš ï¸  {strategy_name}ç­–ç•¥å¤±è´¥: {str(e)[:100]}")
                continue
        
        print("âŒ æ‰€æœ‰PDFåŠ è½½ç­–ç•¥éƒ½å¤±è´¥")
        return []
    
    def process_pdf(self, pdf_path: Path) -> List[Document]:
        """å¤„ç†PDFæ–‡ä»¶ - æ”¹è¿›ç‰ˆ"""
        print(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {pdf_path.name}")
        print(f"   æ–‡ä»¶å¤§å°: {pdf_path.stat().st_size / 1024:.1f} KB")
        
        try:
            # åŠ è½½PDFï¼ˆä½¿ç”¨å¸¦é‡è¯•çš„åŠ è½½å™¨ï¼‰
            documents = self._load_pdf_with_retry(pdf_path)
            
            if not documents:
                print("âŒ PDFåŠ è½½å¤±è´¥")
                return []
            
            print(f"âœ… PDFè§£ææˆåŠŸï¼Œå¾—åˆ° {len(documents)} ä¸ªå…ƒç´ ")
            
            # åˆå¹¶æ–‡æœ¬å†…å®¹
            full_text = ""
            for doc in documents:
                if hasattr(doc, 'page_content'):
                    text = doc.page_content.strip()
                    if text:
                        full_text += text + "\n\n"
            
            print(f"   æå–æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
            
            # æ ¹æ®ç­–ç•¥åˆ†å‰²
            if SPLIT_STRATEGY == "by_section":
                chunks = self.split_by_legal_structure(full_text)
            else:
                chunks = self.split_generic(full_text)
            
            self.stats["total_chunks"] = len(chunks)
            
            # ä¸ºæ¯ä¸ªchunkæ·»åŠ å”¯ä¸€IDï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„å®ç°ï¼‰
            for i, chunk in enumerate(chunks):
                chunk_metadata = chunk.metadata.copy()
                chunk_metadata["chunk_id"] = f"civil_{i:04d}"
                chunk_metadata["chunk_index"] = i
                chunk_metadata["total_chunks"] = len(chunks)
                chunk.metadata = sanitize_metadata(chunk_metadata)
            
            return chunks
            
        except Exception as e:
            print(f"âŒ PDFå¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def check_dependencies(self) -> bool:
        """æ£€æŸ¥å¿…è¦çš„ä¾èµ–ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„å®ç°ï¼‰"""
        print("ğŸ” æ£€æŸ¥ä¾èµ–...")
        
        required_packages = [
            "langchain_community",
            "langchain_text_splitters",
            "langchain_ollama",
            "chromadb",
            "unstructured"
        ]
        
        missing_required = []
        for package in required_packages:
            try:
                __import__(package.replace("-", "_"))
            except ImportError:
                missing_required.append(package)
        
        if missing_required:
            print("âŒ ç¼ºå°‘å¿…éœ€ä¾èµ–:")
            for package in missing_required:
                print(f"   - {package}")
            print(f"\nè¯·è¿è¡Œ: pip install {' '.join(missing_required)}")
            return False
        
        # æ£€æŸ¥PDFä¸“ç”¨ä¾èµ–
        try:
            import pdfminer
            import unstructured_pytesseract
        except ImportError:
            print("âš ï¸  PDFå¤„ç†ä¾èµ–ä¸å®Œæ•´ï¼Œå»ºè®®å®‰è£…:")
            print("    pip install unstructured[pdf] pdfminer.six unstructured_pytesseract")
        
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
        return True
    
    def test_embeddings_directly(self) -> bool:
        """ç›´æ¥æµ‹è¯• Embeddings è€Œä¸åˆå§‹åŒ–æ•´ä¸ªæµç¨‹"""
        print("\nğŸ§ª ç›´æ¥æµ‹è¯• Ollama Embeddings...")
        
        try:
            # æœ€ç®€å•çš„åˆå§‹åŒ–æ–¹å¼
            embeddings = OllamaEmbeddings(model=EMBED_MODEL)
            
            # æµ‹è¯•å°æ–‡æœ¬
            test_text = "æ°‘æ³•å…¸ç¬¬ä¸€æ¡"
            print(f"   æµ‹è¯•æ–‡æœ¬: '{test_text}'")
            
            vector = embeddings.embed_query(test_text)
            
            if vector and len(vector) > 0:
                print(f"âœ… åµŒå…¥æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(vector)}")
                return True
            else:
                print("âŒ åµŒå…¥è¿”å›ç©ºå‘é‡")
                return False
                
        except Exception as e:
            print(f"âŒ åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def ingest(self, force_recreate: bool = False) -> bool:
        """ä¸»å…¥åº“å‡½æ•°"""
        self.stats["start_time"] = time.time()
        
        print("=" * 70)
        print("ğŸ“š æ°‘æ³•å…¸çŸ¥è¯†åº“æ„å»ºç³»ç»Ÿ")
        print("=" * 70)
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return False
        
        # æ£€æŸ¥ Ollama æœåŠ¡
        print("\nğŸ” æ£€æŸ¥ Ollama æœåŠ¡...")
        if not self.check_ollama_service():
            print("\nğŸ”„ å°è¯•è‡ªåŠ¨å¯åŠ¨ Ollama æœåŠ¡...")
            if not self.start_ollama_service():
                print("âŒ æ— æ³•å¯åŠ¨ Ollama æœåŠ¡ï¼Œè¯·æ‰‹åŠ¨å¯åŠ¨")
                print("ğŸ’¡ Windows: åŒå‡» Ollama åº”ç”¨å›¾æ ‡")
                print("ğŸ’¡ Linux/Mac: è¿è¡Œ 'ollama serve'")
                return False
        
        # ç›´æ¥æµ‹è¯• Embeddings
        if not self.test_embeddings_directly():
            print("âŒ Embeddings æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # æŸ¥æ‰¾PDFæ–‡ä»¶
        pdf_path = self.find_civil_code_pdf()
        if not pdf_path:
            return False
        
        # å¤„ç†PDF
        chunks = self.process_pdf(pdf_path)
        if not chunks:
            print("âŒ æœªç”Ÿæˆæœ‰æ•ˆçš„æ–‡æœ¬å—")
            return False
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   ç”Ÿæˆæ–‡æœ¬å—: {self.stats['total_chunks']}")
        print(f"   æ£€æµ‹åˆ°æ³•æ¡æ•°: {self.stats['total_articles']}")
        if self.stats["sections_found"]:
            print(f"   å‘ç°ç« èŠ‚: {', '.join(self.stats['sections_found'][:5])}")
            if len(self.stats["sections_found"]) > 5:
                print(f"             ...ç­‰ {len(self.stats['sections_found'])} ä¸ªç« èŠ‚")
        
        # åˆå§‹åŒ–Embeddingï¼ˆå¸¦é‡è¯•ï¼‰
        print(f"\nğŸ§  åˆå§‹åŒ–Embeddingæ¨¡å‹: {EMBED_MODEL}")
        embeddings = self.initialize_embeddings_with_retry()
        
        if not embeddings:
            print("âŒ Embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ç®€åŒ–åˆå§‹åŒ–...")
            try:
                # å°è¯•æœ€ç®€å•çš„åˆå§‹åŒ–
                embeddings = OllamaEmbeddings(model=EMBED_MODEL)
                test_vector = embeddings.embed_query("æµ‹è¯•")
                print(f"âœ… ç®€åŒ–åˆå§‹åŒ–æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_vector)}")
            except Exception as e:
                print(f"âŒ ç®€åŒ–åˆå§‹åŒ–ä¹Ÿå¤±è´¥: {e}")
                print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
                print("   1. ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
                print("   2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨: ollama list")
                print("   3. ä¸‹è½½æ¨¡å‹: ollama pull " + EMBED_MODEL.split(':')[0])
                print("   4. å°è¯•å…¶ä»–æ¨¡å‹: nomic-embed-text, all-minilm, mxbai-embed-large")
                return False
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        print(f"\nğŸ“¦ æ„å»ºå‘é‡æ•°æ®åº“...")
        print(f"   å­˜å‚¨ä½ç½®: {CIVIL_VECTOR_DB_DIR}")
        
        try:
            # åˆ›å»ºç›®å½•
            os.makedirs(CIVIL_VECTOR_DB_DIR, exist_ok=True)
            
            # è¿‡æ»¤metadataä¸­çš„å¤æ‚ç±»å‹ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„é‡è¦æ­¥éª¤ï¼‰
            print(f"   ğŸ§¹ è¿‡æ»¤metadataä¸­çš„å¤æ‚ç±»å‹...")
            filtered_chunks = filter_complex_metadata(chunks)
            print(f"   è¿‡æ»¤åå‰©ä½™ {len(filtered_chunks)} ä¸ªæ–‡æœ¬å—")
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®åº“ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶çš„å®ç°ï¼‰
            chroma_db_path = os.path.join(CIVIL_VECTOR_DB_DIR, "chroma.sqlite3")
            
            if os.path.exists(chroma_db_path) and not force_recreate:
                print("   ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰å‘é‡åº“ï¼Œè¿›è¡Œå¢é‡æ›´æ–°...")
                
                vectorstore = Chroma(
                    persist_directory=CIVIL_VECTOR_DB_DIR,
                    embedding_function=embeddings
                )
                
                # åˆ†æ‰¹æ·»åŠ æ–‡æ¡£ï¼Œé¿å…å†…å­˜é—®é¢˜
                batch_size = 50
                total_batches = (len(filtered_chunks) + batch_size - 1) // batch_size
                
                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min((batch_idx + 1) * batch_size, len(filtered_chunks))
                    batch = filtered_chunks[start_idx:end_idx]
                    
                    print(f"     æ·»åŠ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)} ä¸ªæ–‡æ¡£)...")
                    vectorstore.add_documents(documents=batch)
                
                operation = "æ›´æ–°"
            else:
                if os.path.exists(chroma_db_path):
                    print("   å¼ºåˆ¶é‡å»ºå‘é‡åº“...")
                else:
                    print("   åˆ›å»ºæ–°å‘é‡åº“...")
                
                # ä½¿ç”¨ from_documents ä½†åˆ†æ‰¹å¤„ç†
                batch_size = 100
                if len(filtered_chunks) > batch_size:
                    print(f"   âš ï¸  æ–‡æ¡£æ•°é‡è¾ƒå¤š ({len(filtered_chunks)})ï¼Œåˆ†æ‰¹å¤„ç†...")
                    # åˆ†æ‰¹åˆ›å»º
                    vectorstore = None
                    total_batches = (len(filtered_chunks) + batch_size - 1) // batch_size
                    
                    for batch_idx in range(total_batches):
                        start_idx = batch_idx * batch_size
                        end_idx = min((batch_idx + 1) * batch_size, len(filtered_chunks))
                        batch = filtered_chunks[start_idx:end_idx]
                        
                        print(f"     å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch)} ä¸ªæ–‡æ¡£)...")
                        
                        if batch_idx == 0:
                            # ç¬¬ä¸€æ‰¹åˆ›å»ºæ•°æ®åº“
                            vectorstore = Chroma.from_documents(
                                documents=batch,
                                embedding=embeddings,
                                persist_directory=CIVIL_VECTOR_DB_DIR,
                                collection_name="civil_code_collection"
                            )
                        else:
                            # åç»­æ‰¹æ¬¡æ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“
                            vectorstore.add_documents(documents=batch)
                else:
                    # æ–‡æ¡£å°‘ï¼Œç›´æ¥åˆ›å»º
                    vectorstore = Chroma.from_documents(
                        documents=filtered_chunks,
                        embedding=embeddings,
                        persist_directory=CIVIL_VECTOR_DB_DIR,
                        collection_name="civil_code_collection"
                    )
                
                operation = "åˆ›å»º"
            
            # éªŒè¯ï¼ˆå‚è€ƒé€šç”¨æ–‡ä»¶ï¼‰
            count = vectorstore._collection.count()
            print(f"âœ… å‘é‡æ•°æ®åº“{operation}å®Œæˆ")
            print(f"   å­˜å‚¨å‘é‡æ•°: {count}")
            
            # æ˜¾ç¤ºç¤ºä¾‹
            print(f"\nğŸ“ ç¤ºä¾‹æ³•æ¡:")
            sample_chunks = filtered_chunks[:min(3, len(filtered_chunks))]
            for i, chunk in enumerate(sample_chunks):
                article_num = chunk.metadata.get('article_number', 'æœªçŸ¥æ³•æ¡')
                preview = chunk.page_content[:100].replace("\n", " ")
                print(f"   {i+1}. {article_num}: {preview}...")
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å­˜é—®é¢˜
            if "memory" in str(e).lower() or "out of memory" in str(e).lower():
                print("\nğŸ’¡ å†…å­˜ä¸è¶³å»ºè®®:")
                print("   1. å¢åŠ ç³»ç»Ÿå†…å­˜")
                print("   2. å‡å°‘æ‰¹é‡å¤§å°")
                print("   3. ä½¿ç”¨æ›´å°çš„åµŒå…¥æ¨¡å‹")
                print("   4. åˆ†æ‰¹å¤„ç†æ–‡æ¡£")
            
            return False
        
        self.stats["end_time"] = time.time()
        duration = self.stats["end_time"] - self.stats["start_time"]
        
        print(f"\nğŸ‰ æ°‘æ³•å…¸çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
        print(f"   æ€»è€—æ—¶: {duration:.1f} ç§’")
        print(f"   å¤„ç†é€Ÿåº¦: {self.stats['total_chunks']/max(duration, 0.1):.1f} å—/ç§’")
        print(f"   Ollamaé‡è¯•æ¬¡æ•°: {self.stats['ollama_retries']}")
        
        return True

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ°‘æ³•å…¸çŸ¥è¯†åº“æ„å»ºå·¥å…·")
    parser.add_argument("--force", "-f", action="store_true", help="å¼ºåˆ¶é‡å»ºå‘é‡åº“")
    parser.add_argument("--check", "-c", action="store_true", help="ä»…æ£€æŸ¥ä¾èµ–å’Œæ–‡ä»¶")
    parser.add_argument("--test", "-t", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼Œåªè§£æä¸å­˜å‚¨")
    parser.add_argument("--model", "-m", type=str, help="æŒ‡å®šä½¿ç”¨çš„Ollamaæ¨¡å‹")
    parser.add_argument("--test-embeddings", "-e", action="store_true", help="åªæµ‹è¯•EmbeddingsåŠŸèƒ½")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œæ›´æ–°é…ç½®
    if args.model:
        global EMBED_MODEL
        EMBED_MODEL = args.model
        print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {EMBED_MODEL}")
    
    if args.check:
        # æ£€æŸ¥ä¾èµ–å’Œæ–‡ä»¶
        ingestor = CivilCodeIngestor()
        ingestor.check_dependencies()
        
        # æ£€æŸ¥ Ollama æœåŠ¡
        ingestor.check_ollama_service()
        
        pdf_path = ingestor.find_civil_code_pdf()
        if pdf_path:
            print(f"\nâœ… æ‰¾åˆ°æ°‘æ³•å…¸PDF: {pdf_path}")
            print(f"   æ–‡ä»¶å¤§å°: {pdf_path.stat().st_size / 1024:.1f} KB")
        else:
            print("âŒ æœªæ‰¾åˆ°æ°‘æ³•å…¸PDF")
        
        return
    
    if args.test_embeddings:
        # åªæµ‹è¯• Embeddings
        ingestor = CivilCodeIngestor()
        ingestor.check_ollama_service()
        ingestor.test_embeddings_directly()
        return
    
    if args.test:
        # æµ‹è¯•æ¨¡å¼ï¼šåªè§£æä¸å­˜å‚¨
        ingestor = CivilCodeIngestor()
        pdf_path = ingestor.find_civil_code_pdf()
        if pdf_path:
            chunks = ingestor.process_pdf(pdf_path)
            if chunks:
                print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼ç»“æœ:")
                print(f"   è§£ææˆåŠŸï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
                print(f"   æ£€æµ‹åˆ° {ingestor.stats['total_articles']} ä¸ªæ³•æ¡")
                
                # æ˜¾ç¤ºå‰3ä¸ªchunk
                for i, chunk in enumerate(chunks[:3]):
                    print(f"\n   --- Chunk {i+1} ---")
                    print(f"   Metadata: {chunk.metadata}")
                    print(f"   Content preview: {chunk.page_content[:200]}...")
        return
    
    # æ‰§è¡Œå…¥åº“
    ingestor = CivilCodeIngestor()
    success = ingestor.ingest(force_recreate=args.force)
    
    if success:
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   1. å¯åŠ¨æœåŠ¡: python run_civil.py")
        print("   2. è®¿é—®: http://127.0.0.1:8001")
    else:
        print("\nâŒ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        print("   1. Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("   2. æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½: ollama list")
        print("   3. å†…å­˜æ˜¯å¦å……è¶³")
        print("   4. ç³»ç»Ÿèµ„æºæ˜¯å¦è¶³å¤Ÿ")
        sys.exit(1)

if __name__ == "__main__":
    main()